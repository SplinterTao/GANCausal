{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.0\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('torch version:',torch.__version__)\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_size=1\n",
    "Y_size=1\n",
    "num_Samples=1000\n",
    "#data_X=np.random.randn(num_Samples,X_size)\n",
    "#data_Y=np.random.randn(num_Samples,Y_size)\n",
    "data_X=np.random.randn(num_Samples,1)\n",
    "data_Y=np.square(data_X)+np.random.randn(num_Samples,1)/100\n",
    "## From 64 to 32 (or pytorch cannot use it)\n",
    "data_X=np.float32(data_X)\n",
    "data_Y=np.float32(data_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CGAN_train(data_X,data_Y,generator_layer_size=[10,10,10],discriminator_layer_size=[10,10,10],\n",
    "              z_dim=2,learning_rate=1e-3,epochs=1000,batch_size=32):\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, generator_layer_size,z_dim,X_dim,Y_dim):\n",
    "            ## Initialization\n",
    "            super().__init__()\n",
    "            self.z_dim = z_dim\n",
    "            self.X_dim=data_X.shape[1]\n",
    "            self.Y_dim=data_Y.shape[1]\n",
    "            ## Build the model\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(self.z_dim + self.X_dim, generator_layer_size[0]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(generator_layer_size[0], generator_layer_size[1]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(generator_layer_size[1], generator_layer_size[2]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(generator_layer_size[2], self.Y_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "        def forward(self, z, X):\n",
    "            z_reshape = z.view(batch_size, self.z_dim)\n",
    "            combined = torch.cat([z_reshape, X], 1)\n",
    "            out = self.model(combined)\n",
    "            return out.view(batch_size, Y_dim)\n",
    "    \n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, discriminator_layer_size, X_dim,Y_dim):\n",
    "            super().__init__()\n",
    "            self.X_dim=data_X.shape[1]\n",
    "            self.Y_dim=data_Y.shape[1]\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Linear(self.Y_dim+self.X_dim, discriminator_layer_size[0]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Linear(discriminator_layer_size[0], discriminator_layer_size[1]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Linear(discriminator_layer_size[1], discriminator_layer_size[2]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Linear(discriminator_layer_size[2], 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        def forward(self, X, Y):\n",
    "            Y = Y.view(-1, self.Y_dim)\n",
    "            combined = torch.cat([X,Y], 1)\n",
    "            out = self.model(combined)\n",
    "            return out.squeeze()\n",
    "    \n",
    "    ##Define generator, discriminator, loss function and optimizer\n",
    "    X_dim=data_X.shape[1]\n",
    "    Y_dim=data_Y.shape[1]\n",
    "    generator = Generator(generator_layer_size,z_dim,X_dim,Y_dim).to(device)\n",
    "    discriminator = Discriminator(discriminator_layer_size, data_X.shape[1],data_Y.shape[1]).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    \n",
    "    ## Define function for training one epoch\n",
    "    def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "        g_optimizer.zero_grad()\n",
    "        z = Variable(torch.randn(batch_size, z_dim)).to(device) ## Generate Noise\n",
    "        ## Generate Fake Data\n",
    "        fake_X = Variable(torch.randn(batch_size, X_dim)).to(device) \n",
    "        fake_Y = generator(z, fake_X)\n",
    "        ## Sent the fake data to discriminator\n",
    "        validity = discriminator(fake_X, fake_Y)\n",
    "        ## Train Model and Back Propagation\n",
    "        g_loss = criterion(validity, Variable(torch.ones(batch_size)).to(device))\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        return g_loss.data\n",
    "    def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, X,Y):\n",
    "        d_optimizer.zero_grad()\n",
    "        ## Get the Real Data\n",
    "        real_validity = discriminator(X,Y)\n",
    "        real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).to(device))\n",
    "        ## Get the Fake data\n",
    "        z = Variable(torch.randn(batch_size, z_dim)).to(device)\n",
    "        fake_X = Variable(torch.randn(batch_size, X_dim)).to(device)\n",
    "        fake_Y = generator(z, fake_X)\n",
    "        fake_validity = discriminator(fake_X,fake_Y)\n",
    "        fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).to(device)) ## Define Fake Loss\n",
    "        d_loss = real_loss + fake_loss ## Total Loss=Loss from both fake data and real data\n",
    "        ## Back Propagation\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        return d_loss.data\n",
    "    ## Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print('Starting epoch {}...'.format(epoch+1))\n",
    "        iteration=int(data_X.shape[0]/batch_size)\n",
    "        for i in range(0,iteration):\n",
    "            if i==iteration:\n",
    "                myrange=range(i*batch_size,data_X.shape[0]+1)\n",
    "            else:\n",
    "                myrange=range(i*batch_size,(i+1)*batch_size)\n",
    "            ## Get Real Data\n",
    "            real_Y=torch.from_numpy(data_Y[myrange])\n",
    "            real_X=torch.from_numpy(data_X[myrange])\n",
    "            generator.train()\n",
    "            d_loss = discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_X,real_Y)\n",
    "            g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "            z=Variable(torch.randn(batch_size, z_dim)).to(device)\n",
    "            sample = generator(z, real_X).unsqueeze(1).data.cpu() ## Generate a Fake Sample\n",
    "            if epoch==epochs-1:\n",
    "                if i==0:\n",
    "                    prediction=sample.numpy().reshape(batch_size,Y_dim)\n",
    "                if i>0:\n",
    "                    temp=sample.numpy().reshape(batch_size,Y_dim)\n",
    "                    prediction=np.concatenate((prediction,temp))\n",
    "        generator.eval()\n",
    "        print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
    "    \n",
    "    \n",
    "    ## Now we generate data to put into the classifier in the next step\n",
    "    fake_data_combine=np.concatenate((data_X[range(prediction.shape[0]),],prediction),axis=1)\n",
    "    print(fake_data_combine.shape)\n",
    "    real_data_combine=np.concatenate((data_X[range(prediction.shape[0]),],data_Y[range(prediction.shape[0]),]),axis=1)\n",
    "    dataset_for_classification=np.concatenate((fake_data_combine,real_data_combine),axis=0)\n",
    "    labels=np.zeros((1,fake_data_combine.shape[0]))\n",
    "    labels_1=np.ones((1,real_data_combine.shape[0]))\n",
    "    labels=np.concatenate((labels,labels_1))\n",
    "    labels=np.float32(labels)\n",
    "    labels=labels.flatten()\n",
    "    labels=torch.from_numpy(labels)\n",
    "    return(dataset_for_classification,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccef848d8bd24db1c12e5fec03cdafcf27468a59"
   },
   "source": [
    "## Classification Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "83a994dc0acddf5d2a2ed1b706b88f6e308997dd"
   },
   "outputs": [],
   "source": [
    "def classification_network_fit(dataset,labels,dataset_v,labels_v,neural_size=[10,10,10],epochs=10,batch_size=1):\n",
    "    neural_size=[10,10,10]\n",
    "    input_dim=dataset.shape[1]\n",
    "    class NeuralNets(nn.Module):\n",
    "        def __init__(self, discriminator_layer_size, input_dim):\n",
    "            super().__init__()\n",
    "        \n",
    "            self.input_dim=input_dim\n",
    "            self.model = nn.Sequential(\n",
    "            \n",
    "            #nn.Linear(input_dim, neural_size[0]),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(neural_size[0], neural_size[1]),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(neural_size[1], neural_size[2]),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(neural_size[2], 1),\n",
    "            \n",
    "            #nn.Linear(input_dim,1),\n",
    "            \n",
    "            nn.Linear(input_dim, neural_size[0]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),    \n",
    "            nn.Linear(neural_size[0],1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        def forward(self, X):\n",
    "            out = self.model(X)\n",
    "            return out.squeeze()\n",
    "    model_c=NeuralNets(neural_size,dataset.shape[1],)\n",
    "    loss_for_classification=torch.nn.BCELoss()\n",
    "    optimizer_for_classification=torch.optim.SGD(model_c.parameters(), lr=0.01)\n",
    "    for epoch in range(20):\n",
    "        print('Starting epoch {}...'.format(epoch+1))\n",
    "        iteration=int(data_X.shape[0]/batch_size)\n",
    "        for i in range(0,iteration):\n",
    "            data=torch.from_numpy(dataset)\n",
    "            optimizer_for_classification.zero_grad()\n",
    "            outputs=model_c(data)\n",
    "            loss_c=loss_for_classification(outputs,labels)\n",
    "            loss_c.backward()\n",
    "            optimizer_for_classification.step()\n",
    "        \n",
    "    \n",
    "    \n",
    "    prediction=model_c(torch.from_numpy(dataset_v))\n",
    "    return prediction,labels_v\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def adversarial_causal_test(x,y,epochs=1000):\n",
    "    data,label=CGAN_train(x,y,epochs=epochs)\n",
    "    X_dim=x.shape[1]\n",
    "    ## We will only keep Y but not X\n",
    "    data=np.delete(data,range(X_dim),1)\n",
    "    \n",
    "    ## Train-test split\n",
    "    training_index=list(random.sample(range(0,data.shape[0]),int(data.shape[0]*0.8)))\n",
    "    validation_index=list(set(range(0,data.shape[0]))-set(training_index))\n",
    "    data_Y_train=data[training_index,:]\n",
    "    label_train=label[training_index]\n",
    "    data_Y_validation=data[validation_index,:]\n",
    "    label_validation=label[validation_index]\n",
    "    \n",
    "    ## Send the result to classifier\n",
    "    prediction_v,labels_v=classification_network_fit(data_Y_train,label_train,data_Y_validation,label_validation)\n",
    "    \n",
    "    ## see how well the classifier performs\n",
    "    result=[]\n",
    "    for i in range(0,prediction_v.shape[0]):\n",
    "        if ((prediction_v[i]>0.5) and (labels_v[i]==1))or ((prediction_v[i]<0.5) and (labels_v[i]==0)):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result,prediction_v.shape[0]\n",
    "    ## Adversarial Causal Tests\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import csv\n",
    "import random\n",
    "\n",
    "def get_list(mydata,index):\n",
    "    mylist=[]\n",
    "    for i in index:\n",
    "        mylist.append(mydata[i])\n",
    "    return mylist\n",
    "\n",
    "def get_test_statistic(data_X,data_Y):\n",
    "    XtoY,len1=adversarial_causal_test(data_X,data_Y)\n",
    "    YtoX,len2=adversarial_causal_test(data_Y,data_X)\n",
    "    TCXtoY=sum(np.asarray(XtoY))/len(XtoY)\n",
    "    \n",
    "    TCYtoX=sum(np.asarray(YtoX))/len(YtoX)\n",
    "    \n",
    "    ## Theoretical Variance\n",
    "    sigmasquared=2*(1/(4*len(XtoY))-sum((XtoY-TCXtoY)*(YtoX-TCYtoX)/(len(XtoY)-1)))\n",
    "    \n",
    "    \n",
    "    ## Bootstrap variance\n",
    "    difference_set=[]\n",
    "    for i in range(0,100):\n",
    "        index=random.choices(list(range(0,len(XtoY))),k=len(XtoY))\n",
    "        XtoYstar=get_list(XtoY,index)\n",
    "        YtoXstar=get_list(YtoX,index)\n",
    "        difference=sum(np.asarray(XtoYstar))/len(XtoY)-sum(np.asarray(YtoXstar))/len(YtoX)\n",
    "        difference_set.append(difference)\n",
    "    sigmasquared_b=np.var(difference_set)\n",
    "    TCE=(TCXtoY-TCYtoX)**2/sigmasquared\n",
    "    TCE_b=(TCXtoY-TCYtoX)**2/sigmasquared_b\n",
    "    \n",
    "    return TCXtoY, TCYtoX, TCE, TCE_b, sigmasquared, sigmasquared_b\n",
    "\n",
    "#A=get_test_statistic(data_X,data_Y)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('result3.csv','w',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in range(100):\n",
    "        data_X=np.random.randn(num_Samples,1)\n",
    "        data_Y=np.power(data_X,3)+np.random.randn(num_Samples,1)/10\n",
    "        data_X=np.float32(data_X)\n",
    "        data_Y=np.float32(data_Y)\n",
    "        result=get_test_statistic(data_X,data_Y)\n",
    "        writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[207,\n",
       " 128,\n",
       " 353,\n",
       " 166,\n",
       " 384,\n",
       " 198,\n",
       " 285,\n",
       " 265,\n",
       " 165,\n",
       " 394,\n",
       " 89,\n",
       " 292,\n",
       " 364,\n",
       " 107,\n",
       " 370,\n",
       " 37,\n",
       " 207,\n",
       " 170,\n",
       " 13,\n",
       " 130,\n",
       " 86,\n",
       " 113,\n",
       " 159,\n",
       " 178,\n",
       " 29,\n",
       " 314,\n",
       " 185,\n",
       " 383,\n",
       " 238,\n",
       " 138,\n",
       " 290,\n",
       " 195,\n",
       " 373,\n",
       " 325,\n",
       " 217,\n",
       " 85,\n",
       " 392,\n",
       " 242,\n",
       " 244,\n",
       " 201,\n",
       " 182,\n",
       " 227,\n",
       " 200,\n",
       " 77,\n",
       " 268,\n",
       " 6,\n",
       " 49,\n",
       " 303,\n",
       " 309,\n",
       " 225,\n",
       " 0,\n",
       " 356,\n",
       " 208,\n",
       " 148,\n",
       " 249,\n",
       " 376,\n",
       " 385,\n",
       " 373,\n",
       " 358,\n",
       " 363,\n",
       " 377,\n",
       " 306,\n",
       " 362,\n",
       " 221,\n",
       " 17,\n",
       " 287,\n",
       " 31,\n",
       " 182,\n",
       " 117,\n",
       " 222,\n",
       " 159,\n",
       " 323,\n",
       " 284,\n",
       " 79,\n",
       " 275,\n",
       " 16,\n",
       " 0,\n",
       " 55,\n",
       " 225,\n",
       " 108,\n",
       " 158,\n",
       " 322,\n",
       " 124,\n",
       " 7,\n",
       " 209,\n",
       " 258,\n",
       " 158,\n",
       " 221,\n",
       " 187,\n",
       " 172,\n",
       " 308,\n",
       " 337,\n",
       " 105,\n",
       " 211,\n",
       " 234,\n",
       " 108,\n",
       " 378,\n",
       " 323,\n",
       " 90,\n",
       " 380,\n",
       " 28,\n",
       " 324,\n",
       " 307,\n",
       " 66,\n",
       " 189,\n",
       " 303,\n",
       " 170,\n",
       " 297,\n",
       " 89,\n",
       " 340,\n",
       " 199,\n",
       " 68,\n",
       " 138,\n",
       " 188,\n",
       " 143,\n",
       " 271,\n",
       " 302,\n",
       " 171,\n",
       " 342,\n",
       " 304,\n",
       " 294,\n",
       " 60,\n",
       " 119,\n",
       " 199,\n",
       " 234,\n",
       " 368,\n",
       " 331,\n",
       " 101,\n",
       " 299,\n",
       " 153,\n",
       " 376,\n",
       " 72,\n",
       " 295,\n",
       " 27,\n",
       " 87,\n",
       " 285,\n",
       " 202,\n",
       " 365,\n",
       " 76,\n",
       " 67,\n",
       " 96,\n",
       " 240,\n",
       " 301,\n",
       " 59,\n",
       " 136,\n",
       " 176,\n",
       " 127,\n",
       " 35,\n",
       " 340,\n",
       " 343,\n",
       " 55,\n",
       " 187,\n",
       " 17,\n",
       " 383,\n",
       " 29,\n",
       " 62,\n",
       " 301,\n",
       " 51,\n",
       " 117,\n",
       " 67,\n",
       " 137,\n",
       " 236,\n",
       " 368,\n",
       " 15,\n",
       " 363,\n",
       " 112,\n",
       " 313,\n",
       " 197,\n",
       " 256,\n",
       " 353,\n",
       " 395,\n",
       " 362,\n",
       " 12,\n",
       " 2,\n",
       " 104,\n",
       " 108,\n",
       " 281,\n",
       " 383,\n",
       " 315,\n",
       " 248,\n",
       " 252,\n",
       " 143,\n",
       " 138,\n",
       " 142,\n",
       " 108,\n",
       " 77,\n",
       " 282,\n",
       " 68,\n",
       " 113,\n",
       " 46,\n",
       " 35,\n",
       " 154,\n",
       " 187,\n",
       " 328,\n",
       " 79,\n",
       " 15,\n",
       " 255,\n",
       " 197,\n",
       " 381,\n",
       " 312,\n",
       " 340,\n",
       " 40,\n",
       " 294,\n",
       " 304,\n",
       " 339,\n",
       " 365,\n",
       " 361,\n",
       " 296,\n",
       " 177,\n",
       " 380,\n",
       " 103,\n",
       " 301,\n",
       " 306,\n",
       " 74,\n",
       " 334,\n",
       " 43,\n",
       " 40,\n",
       " 344,\n",
       " 57,\n",
       " 202,\n",
       " 383,\n",
       " 122,\n",
       " 250,\n",
       " 2,\n",
       " 273,\n",
       " 263,\n",
       " 168,\n",
       " 259,\n",
       " 246,\n",
       " 277,\n",
       " 206,\n",
       " 309,\n",
       " 5,\n",
       " 124,\n",
       " 178,\n",
       " 210,\n",
       " 236,\n",
       " 76,\n",
       " 71,\n",
       " 189,\n",
       " 225,\n",
       " 55,\n",
       " 341,\n",
       " 392,\n",
       " 308,\n",
       " 27,\n",
       " 284,\n",
       " 197,\n",
       " 204,\n",
       " 327,\n",
       " 297,\n",
       " 307,\n",
       " 162,\n",
       " 166,\n",
       " 213,\n",
       " 139,\n",
       " 114,\n",
       " 325,\n",
       " 10,\n",
       " 274,\n",
       " 112,\n",
       " 244,\n",
       " 297,\n",
       " 352,\n",
       " 132,\n",
       " 367,\n",
       " 314,\n",
       " 318,\n",
       " 50,\n",
       " 186,\n",
       " 160,\n",
       " 255,\n",
       " 283,\n",
       " 71,\n",
       " 117,\n",
       " 199,\n",
       " 300,\n",
       " 185,\n",
       " 295,\n",
       " 354,\n",
       " 369,\n",
       " 389,\n",
       " 94,\n",
       " 295,\n",
       " 281,\n",
       " 134,\n",
       " 131,\n",
       " 145,\n",
       " 348,\n",
       " 54,\n",
       " 368,\n",
       " 346,\n",
       " 300,\n",
       " 268,\n",
       " 304,\n",
       " 247,\n",
       " 101,\n",
       " 387,\n",
       " 124,\n",
       " 85,\n",
       " 393,\n",
       " 180,\n",
       " 91,\n",
       " 333,\n",
       " 12,\n",
       " 177,\n",
       " 253,\n",
       " 198,\n",
       " 141,\n",
       " 97,\n",
       " 55,\n",
       " 293,\n",
       " 97,\n",
       " 255,\n",
       " 160,\n",
       " 37,\n",
       " 309,\n",
       " 59,\n",
       " 376,\n",
       " 116,\n",
       " 10,\n",
       " 308,\n",
       " 367,\n",
       " 130,\n",
       " 197,\n",
       " 135,\n",
       " 17,\n",
       " 233,\n",
       " 101,\n",
       " 231,\n",
       " 173,\n",
       " 255,\n",
       " 35,\n",
       " 286,\n",
       " 111,\n",
       " 54,\n",
       " 291,\n",
       " 391,\n",
       " 108,\n",
       " 162,\n",
       " 121,\n",
       " 178,\n",
       " 368,\n",
       " 379,\n",
       " 272,\n",
       " 180,\n",
       " 314,\n",
       " 383,\n",
       " 335,\n",
       " 143,\n",
       " 11,\n",
       " 222,\n",
       " 257,\n",
       " 220,\n",
       " 169,\n",
       " 144,\n",
       " 192,\n",
       " 141,\n",
       " 158,\n",
       " 342,\n",
       " 362,\n",
       " 97,\n",
       " 375,\n",
       " 362,\n",
       " 260,\n",
       " 384,\n",
       " 222,\n",
       " 268,\n",
       " 87,\n",
       " 380,\n",
       " 99,\n",
       " 0,\n",
       " 270,\n",
       " 145,\n",
       " 120,\n",
       " 257,\n",
       " 106,\n",
       " 266,\n",
       " 359,\n",
       " 125,\n",
       " 96,\n",
       " 284,\n",
       " 187,\n",
       " 74,\n",
       " 32,\n",
       " 228,\n",
       " 13,\n",
       " 157,\n",
       " 37,\n",
       " 393,\n",
       " 307,\n",
       " 142,\n",
       " 65,\n",
       " 56,\n",
       " 366,\n",
       " 257,\n",
       " 58]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.0 0.9319899244332494 3.672544080604536 32.34881698290707 0.0012594458438287153 0.00014298422044426394</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0 0.9017632241813602 7.6624685138538995 49.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0 0.9596977329974811 1.2896725440806072 14.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0 0.9093198992443325 6.5289672544080535 45.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0 0.929471032745592 3.94962216624685 36.3057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0 0.9319899244332494 3.672544080604536 29.91...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0 0.8942065491183879 8.886649874055413 45.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0 0.8942065491183879 8.886649874055413 50.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0 0.9370277078085643 3.1486146095717853 20.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0 0.924433249370277 4.53400503778338 27.7695...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0 0.9445843828715366 2.4382871536523893 21.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1.0 0.9319899244332494 3.672544080604536 32.34881698290707 0.0012594458438287153 0.00014298422044426394\n",
       "0   1.0 0.9017632241813602 7.6624685138538995 49.5...                                                     \n",
       "1   1.0 0.9596977329974811 1.2896725440806072 14.8...                                                     \n",
       "2   1.0 0.9093198992443325 6.5289672544080535 45.4...                                                     \n",
       "3   1.0 0.929471032745592 3.94962216624685 36.3057...                                                     \n",
       "4   1.0 0.9319899244332494 3.672544080604536 29.91...                                                     \n",
       "..                                                ...                                                     \n",
       "94  1.0 0.8942065491183879 8.886649874055413 45.26...                                                     \n",
       "95  1.0 0.8942065491183879 8.886649874055413 50.16...                                                     \n",
       "96  1.0 0.9370277078085643 3.1486146095717853 20.6...                                                     \n",
       "97  1.0 0.924433249370277 4.53400503778338 27.7695...                                                     \n",
       "98  1.0 0.9445843828715366 2.4382871536523893 21.4...                                                     \n",
       "\n",
       "[99 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "PATH = \"/Users/taoxu/Desktop/conditionalGAN/conditional-GAN-main/default\"\n",
    "EXT = \"*.csv\"\n",
    "all_csv_files = [file\n",
    "                 for path, subdir, files in os.walk(PATH)\n",
    "                 for file in glob(os.path.join(path, EXT))]\n",
    "#for i in all_csv_files:\n",
    "    \n",
    "results=pd.read_csv(all_csv_files[0])\n",
    "results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/taoxu/Desktop/conditionalGAN/conditional-GAN-main'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b9/w4wr95zj6xd_8wgpnqk78b2m0000gn/T/ipykernel_98869/1723008382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXtoY\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mYtoX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "XtoY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YtoX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
